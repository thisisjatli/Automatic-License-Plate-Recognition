{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1653901260162,
     "user": {
      "displayName": "李庭逸",
      "userId": "17692844335573755132"
     },
     "user_tz": -480
    },
    "id": "SV2ZVMS0D4B6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1653901260163,
     "user": {
      "displayName": "李庭逸",
      "userId": "17692844335573755132"
     },
     "user_tz": -480
    },
    "id": "kIP0AmA3kU98",
    "outputId": "ff24d2a0-3565-49ae-c8f8-78b68342884a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653901260163,
     "user": {
      "displayName": "李庭逸",
      "userId": "17692844335573755132"
     },
     "user_tz": -480
    },
    "id": "hbhjQqgH6QCy"
   },
   "outputs": [],
   "source": [
    "class convBlock(nn.Module):\n",
    "  def __init__(self, input_node, output_node, filter_size):\n",
    "    super(convBlock, self).__init__()\n",
    "    self.conv = nn.Conv2d(input_node, output_node, (filter_size, filter_size), padding=(5, 5)) # padding problem\n",
    "    self.maxpool = nn.MaxPool2d((2, 2))\n",
    "  \n",
    "  def forward(self, input):\n",
    "    x = self.conv(input)\n",
    "    x = self.maxpool(x)\n",
    "    return x\n",
    "\n",
    "# class flatBlock(nn.module):\n",
    "#   def __init__(self):\n",
    "#     self.flat = nn.Flatten()\n",
    "#     self.dense = nn.Linear(128, 64) # not sure about input size 128\n",
    "\n",
    "class mymodel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(mymodel, self).__init__()\n",
    "    # self.trans = nn.Sequential(transforms.Resize(48), transforms.Grayscale()) # transform image\n",
    "    self.conv1 = convBlock(1, 16, 22)\n",
    "    self.conv2 = convBlock(16, 32, 16)\n",
    "    self.conv3 = convBlock(32, 64, 8)\n",
    "    self.conv4 = convBlock(64, 64, 4)\n",
    "    # self.maxpool = nn.MaxPool2d((4, 4)) # use stride?\n",
    "    self.batchnorm = nn.BatchNorm2d(64)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.drop = nn.Dropout(0.4)\n",
    "    self.flat = nn.Flatten()\n",
    "    # self.line1 = nn.Linear(2304, 1000)\n",
    "    # self.drop2 = nn.Dropout(0.4)\n",
    "    # self.line2 = nn.Linear(1000, 500)\n",
    "    # self.line3 = nn.Linear(500, 100)\n",
    "    self.classifier = nn.Linear(2304, 35) # not sure about input size 128\n",
    "    # self.linelayers = nn.Sequential(self.flat, self.line1, self.drop2, self.line2, self.line3, self.classifier)\n",
    "    self.layers = nn.Sequential(self.conv1, self.conv2, self.conv3, self.conv4, self.batchnorm, self.relu, self.drop, self.flat, self.classifier)\n",
    "\n",
    "  def forward(self, input):\n",
    "    output = self.layers(input)\n",
    "    # print(\"forward done\")\n",
    "    return output\n",
    "\n",
    "class kaggle_1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(kaggle_1, self).__init__()\n",
    "    self.flat = nn.Flatten()\n",
    "    self.l1 = nn.Linear(1280, 128)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.bn = nn.BatchNorm1d(128)\n",
    "    self.l2 = nn.Linear(128, 128)\n",
    "    self.l3 = nn.Linear(128, 64)\n",
    "    self.l4 = nn.Linear(64, 35)\n",
    "    self.softmax = nn.Softmax()\n",
    "    self.layers = nn.Sequential(self.flat, self.l1, self.relu, self.bn, self.l2, self.relu, self.bn, self.l3, self.relu, self.l4, self.softmax)\n",
    "\n",
    "  def forward(self, input):\n",
    "    output = self.layers(input)\n",
    "    # print(\"forward done\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1653901260163,
     "user": {
      "displayName": "李庭逸",
      "userId": "17692844335573755132"
     },
     "user_tz": -480
    },
    "id": "iOgMcMvlKEbN"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, device, val_dl, criterion):\n",
    "  model.eval()\n",
    "  loss = 0\n",
    "  acc = 0\n",
    "  with torch.no_grad():\n",
    "    for x_val, y_val in val_dl:\n",
    "      x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "      output = model(x_val)\n",
    "      pred = torch.argmax(output, axis=1)\n",
    "      loss += criterion(output, y_val)\n",
    "      acc += (pred==y_val).sum().item()\n",
    "\n",
    "  return loss/len(val_dl), acc/len(val_dl.dataset)\n",
    "\n",
    "def train_model(model, train_dl, val_dl, device, path):\n",
    "  model_name = 'mymodel'\n",
    "  lr = 0.0001\n",
    "  weight_decay = 0.0001\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  epochs = 200\n",
    "  record = {'train_loss':[], 'train_accuracy':[], 'val_loss':[], 'val_accuracy':[]}\n",
    "\n",
    "  max_acc = 0\n",
    "  print(\"Training start...\")\n",
    "  torch.cuda.synchronize()\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "\n",
    "    for x_train, y_train in train_dl:\n",
    "      x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(x_train)\n",
    "      pred = torch.argmax(output, axis=1)\n",
    "      loss = criterion(output, y_train)\n",
    "      train_loss += loss.detach().cpu().item()\n",
    "      train_accuracy += (pred==y_train).sum().item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "\n",
    "    train_loss = train_loss/len(train_dl)\n",
    "    train_accuracy = train_accuracy/len(train_dl.dataset)\n",
    "    record['train_loss'].append(train_loss) # len(train_dl) = total number of data / batch size\n",
    "    record['train_accuracy'].append(train_accuracy)\n",
    "\n",
    "    \n",
    "    # evaluate model\n",
    "    val_loss, val_accuracy = evaluate_model(model, device, val_dl, criterion)\n",
    "    if val_accuracy > max_acc:\n",
    "      max_acc = val_accuracy\n",
    "      torch.save(model.state_dict(), os.path.join(path, f'{model_name}_combinedThree_reduced_bs{BATCH_SIZE}_e{epochs}_lr{lr}_wd{weight_decay}'))\n",
    "\n",
    "    record['val_loss'].append(val_loss)\n",
    "    record['val_accuracy'].append(val_accuracy)\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'epoch: {epoch}, train_loss: {train_loss}, train_acc: {train_accuracy}, val_loss: {val_loss}, val_acc: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1653901260163,
     "user": {
      "displayName": "李庭逸",
      "userId": "17692844335573755132"
     },
     "user_tz": -480
    },
    "id": "ga6hG2qmlyhl"
   },
   "outputs": [],
   "source": [
    "def loadData(filepath):\n",
    "    trans = transforms.Compose([transforms.Resize((64, 64)), transforms.Grayscale(), transforms.ToTensor()])\n",
    "    dataset = datasets.ImageFolder(filepath, transform=trans) # resize?\n",
    "\n",
    "    # randomly split train and validate\n",
    "    train_set, val_set = random_split(dataset, [int(0.9*len(dataset)), int(0.1*len(dataset))])\n",
    "    # torch.save(train_set, 'tensorData/train_set_64_64.pt')\n",
    "    # torch.save(val_set, 'tensorData/val_set_64_64.pt')\n",
    "\n",
    "    #train_set = torch.load('tensorData/train_set_64_64.pt')\n",
    "    #val_set = torch.load('tensorData/val_set_64_64.pt')\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsMjL1oBsXGm",
    "outputId": "3b482bb9-a3fa-444d-ddde-68d4efa1b25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start...\n",
      "epoch: 0, train_loss: 1.4604169795679491, train_acc: 0.70441400304414, val_loss: 0.5593900084495544, val_acc: 0.9246575342465754\n",
      "epoch: 1, train_loss: 0.3838237563306971, train_acc: 0.9397564687975647, val_loss: 0.2749934494495392, val_acc: 0.9671232876712329\n",
      "epoch: 2, train_loss: 0.20166345599085786, train_acc: 0.9698325722983258, val_loss: 0.15188176929950714, val_acc: 0.9802739726027397\n",
      "epoch: 3, train_loss: 0.12303495476412218, train_acc: 0.9837747336377474, val_loss: 0.09558787941932678, val_acc: 0.9887671232876712\n",
      "epoch: 4, train_loss: 0.08017865074581879, train_acc: 0.9913850837138508, val_loss: 0.060854729264974594, val_acc: 0.9953424657534247\n",
      "epoch: 5, train_loss: 0.05784897272323453, train_acc: 0.9952511415525114, val_loss: 0.0415707528591156, val_acc: 0.9980821917808219\n",
      "epoch: 6, train_loss: 0.041233400258329486, train_acc: 0.9969254185692542, val_loss: 0.03645249456167221, val_acc: 0.9980821917808219\n",
      "epoch: 7, train_loss: 0.032164520432436186, train_acc: 0.9982039573820396, val_loss: 0.020839599892497063, val_acc: 0.9991780821917808\n",
      "epoch: 8, train_loss: 0.02530232915842487, train_acc: 0.9986910197869102, val_loss: 0.020943772047758102, val_acc: 0.9994520547945206\n",
      "epoch: 9, train_loss: 0.020574018878992215, train_acc: 0.9989041095890411, val_loss: 0.015086065046489239, val_acc: 0.9994520547945206\n",
      "epoch: 10, train_loss: 0.01592653184811505, train_acc: 0.9995129375951294, val_loss: 0.0138765973970294, val_acc: 0.9989041095890411\n",
      "epoch: 11, train_loss: 0.014014355953811675, train_acc: 0.9994216133942161, val_loss: 0.011329815723001957, val_acc: 0.9994520547945206\n",
      "epoch: 12, train_loss: 0.011377296720941862, train_acc: 0.9996651445966515, val_loss: 0.009732197970151901, val_acc: 0.9994520547945206\n",
      "epoch: 13, train_loss: 0.009444296894262927, train_acc: 0.9997260273972602, val_loss: 0.006724516395479441, val_acc: 0.9994520547945206\n",
      "epoch: 14, train_loss: 0.00781132675014263, train_acc: 0.9998477929984779, val_loss: 0.0064940075390040874, val_acc: 0.9994520547945206\n",
      "epoch: 15, train_loss: 0.006887613925649677, train_acc: 0.9999391171993912, val_loss: 0.0053758989088237286, val_acc: 0.9994520547945206\n",
      "epoch: 16, train_loss: 0.00613453879396525, train_acc: 0.9999391171993912, val_loss: 0.005265859887003899, val_acc: 0.9994520547945206\n",
      "epoch: 17, train_loss: 0.005532509163599606, train_acc: 0.9999391171993912, val_loss: 0.005023160949349403, val_acc: 0.9994520547945206\n",
      "epoch: 18, train_loss: 0.004517194125512543, train_acc: 1.0, val_loss: 0.003860394237563014, val_acc: 0.9997260273972602\n",
      "epoch: 19, train_loss: 0.004352404095246926, train_acc: 0.9999391171993912, val_loss: 0.0034962112549692392, val_acc: 0.9997260273972602\n",
      "epoch: 20, train_loss: 0.003938515300595367, train_acc: 1.0, val_loss: 0.003527282737195492, val_acc: 0.9997260273972602\n",
      "epoch: 21, train_loss: 0.0032299046662758834, train_acc: 1.0, val_loss: 0.002869728719815612, val_acc: 1.0\n",
      "epoch: 22, train_loss: 0.003430907477357591, train_acc: 0.9999086757990867, val_loss: 0.002491476945579052, val_acc: 1.0\n",
      "epoch: 23, train_loss: 0.0027943255583748453, train_acc: 1.0, val_loss: 0.002331267576664686, val_acc: 0.9997260273972602\n",
      "epoch: 24, train_loss: 0.002508219396027686, train_acc: 1.0, val_loss: 0.002499277237802744, val_acc: 0.9994520547945206\n",
      "epoch: 25, train_loss: 0.0031078501573814326, train_acc: 0.9999086757990867, val_loss: 0.0022460187319666147, val_acc: 0.9997260273972602\n",
      "epoch: 26, train_loss: 0.0024564297876118633, train_acc: 1.0, val_loss: 0.0024160288739949465, val_acc: 0.9997260273972602\n",
      "epoch: 27, train_loss: 0.0020583529104731342, train_acc: 1.0, val_loss: 0.0017881016246974468, val_acc: 0.9997260273972602\n",
      "epoch: 28, train_loss: 0.0015409643781667415, train_acc: 1.0, val_loss: 0.001773487194441259, val_acc: 0.9994520547945206\n",
      "epoch: 29, train_loss: 0.001514189995738656, train_acc: 1.0, val_loss: 0.0013317442499101162, val_acc: 0.9997260273972602\n",
      "epoch: 30, train_loss: 0.0014296358134489246, train_acc: 1.0, val_loss: 0.0014094209764152765, val_acc: 0.9997260273972602\n",
      "epoch: 31, train_loss: 0.0015074050278662769, train_acc: 1.0, val_loss: 0.0013755694963037968, val_acc: 0.9997260273972602\n",
      "epoch: 32, train_loss: 0.0013755194038737478, train_acc: 0.9999695585996956, val_loss: 0.0014146449975669384, val_acc: 0.9997260273972602\n",
      "epoch: 33, train_loss: 0.0014375816992765597, train_acc: 1.0, val_loss: 0.0015374907525256276, val_acc: 0.9997260273972602\n",
      "epoch: 34, train_loss: 0.0011964399252960443, train_acc: 1.0, val_loss: 0.0013031743001192808, val_acc: 0.9997260273972602\n",
      "epoch: 35, train_loss: 0.00120227860017001, train_acc: 1.0, val_loss: 0.0021007643081247807, val_acc: 0.9997260273972602\n",
      "epoch: 36, train_loss: 0.0019409403691335416, train_acc: 0.9999695585996956, val_loss: 0.0018904849421232939, val_acc: 0.9994520547945206\n",
      "epoch: 37, train_loss: 0.0012496981405212668, train_acc: 1.0, val_loss: 0.0014625737676396966, val_acc: 0.9997260273972602\n",
      "epoch: 38, train_loss: 0.001015053496176247, train_acc: 1.0, val_loss: 0.000882662832736969, val_acc: 1.0\n",
      "epoch: 39, train_loss: 0.00939478390342953, train_acc: 0.9989041095890411, val_loss: 0.0052855429239571095, val_acc: 0.9991780821917808\n",
      "epoch: 40, train_loss: 0.002524238990352655, train_acc: 0.9999086757990867, val_loss: 0.0011496349470689893, val_acc: 0.9997260273972602\n",
      "epoch: 41, train_loss: 0.0015336340455728214, train_acc: 0.9999695585996956, val_loss: 0.0010285378666594625, val_acc: 0.9997260273972602\n",
      "epoch: 42, train_loss: 0.0009438733332483604, train_acc: 1.0, val_loss: 0.0011003154795616865, val_acc: 0.9997260273972602\n",
      "epoch: 43, train_loss: 0.00178727925829084, train_acc: 0.9998173515981735, val_loss: 0.0034487477969378233, val_acc: 0.9997260273972602\n",
      "epoch: 44, train_loss: 0.0040719187096512136, train_acc: 0.9994520547945206, val_loss: 0.0028483790811151266, val_acc: 1.0\n",
      "epoch: 45, train_loss: 0.0013197745634658017, train_acc: 1.0, val_loss: 0.0005904803983867168, val_acc: 1.0\n",
      "epoch: 46, train_loss: 0.0005683351568462572, train_acc: 1.0, val_loss: 0.0005462901317514479, val_acc: 1.0\n",
      "epoch: 47, train_loss: 0.0004160630665836827, train_acc: 1.0, val_loss: 0.0005517415702342987, val_acc: 1.0\n",
      "epoch: 48, train_loss: 0.0004036314383892761, train_acc: 1.0, val_loss: 0.0004791399114765227, val_acc: 1.0\n",
      "epoch: 49, train_loss: 0.00037086613940132796, train_acc: 1.0, val_loss: 0.0004609536554198712, val_acc: 1.0\n",
      "epoch: 50, train_loss: 0.0003803627556027455, train_acc: 1.0, val_loss: 0.0006541017792187631, val_acc: 1.0\n",
      "epoch: 51, train_loss: 0.0003893919430795855, train_acc: 1.0, val_loss: 0.000437720213085413, val_acc: 1.0\n",
      "epoch: 52, train_loss: 0.00035043734251232747, train_acc: 1.0, val_loss: 0.00043203053064644337, val_acc: 1.0\n",
      "epoch: 53, train_loss: 0.00031255646967491525, train_acc: 1.0, val_loss: 0.0004481594078242779, val_acc: 1.0\n",
      "epoch: 54, train_loss: 0.0002991877071642454, train_acc: 1.0, val_loss: 0.0005474102217704058, val_acc: 0.9997260273972602\n",
      "epoch: 55, train_loss: 0.00031124548210452, train_acc: 1.0, val_loss: 0.0004215053340885788, val_acc: 1.0\n",
      "epoch: 56, train_loss: 0.0003063159168141988, train_acc: 1.0, val_loss: 0.0004582923138514161, val_acc: 1.0\n",
      "epoch: 57, train_loss: 0.00032665957462100326, train_acc: 1.0, val_loss: 0.0005187983624637127, val_acc: 1.0\n",
      "epoch: 58, train_loss: 0.00033258240671770633, train_acc: 1.0, val_loss: 0.0004914459423162043, val_acc: 1.0\n",
      "epoch: 59, train_loss: 0.0003331451668438336, train_acc: 1.0, val_loss: 0.0004286658368073404, val_acc: 1.0\n",
      "epoch: 60, train_loss: 0.0003266307952192922, train_acc: 1.0, val_loss: 0.0006429904606193304, val_acc: 0.9997260273972602\n",
      "epoch: 61, train_loss: 0.0005992081699001264, train_acc: 1.0, val_loss: 0.0007057617767713964, val_acc: 0.9997260273972602\n",
      "epoch: 62, train_loss: 0.00044143730185676884, train_acc: 1.0, val_loss: 0.0005930782062932849, val_acc: 1.0\n",
      "epoch: 63, train_loss: 0.016495933609810665, train_acc: 0.9970167427701674, val_loss: 0.0028768591582775116, val_acc: 1.0\n",
      "epoch: 64, train_loss: 0.0016308345528709333, train_acc: 0.9999695585996956, val_loss: 0.000916493940167129, val_acc: 1.0\n",
      "epoch: 65, train_loss: 0.0011200374328484398, train_acc: 0.9999086757990867, val_loss: 0.000811378937214613, val_acc: 0.9997260273972602\n",
      "epoch: 66, train_loss: 0.0010008406724966701, train_acc: 0.9999391171993912, val_loss: 0.0006106232176534832, val_acc: 1.0\n",
      "epoch: 67, train_loss: 0.0004832876688040894, train_acc: 1.0, val_loss: 0.000428295781603083, val_acc: 1.0\n",
      "epoch: 68, train_loss: 0.0004158964712947044, train_acc: 1.0, val_loss: 0.0005186986527405679, val_acc: 1.0\n",
      "epoch: 69, train_loss: 0.00032927525464838817, train_acc: 1.0, val_loss: 0.00044144332059659064, val_acc: 1.0\n",
      "epoch: 70, train_loss: 0.0003565260806561325, train_acc: 1.0, val_loss: 0.0004255120293237269, val_acc: 1.0\n",
      "epoch: 71, train_loss: 0.0002837012990742431, train_acc: 1.0, val_loss: 0.0003840915160253644, val_acc: 1.0\n",
      "epoch: 72, train_loss: 0.00029354578930011033, train_acc: 1.0, val_loss: 0.00039174637640826404, val_acc: 1.0\n",
      "epoch: 73, train_loss: 0.0002913184307403718, train_acc: 1.0, val_loss: 0.00040758351678960025, val_acc: 1.0\n",
      "epoch: 74, train_loss: 0.0002692412897942646, train_acc: 1.0, val_loss: 0.0004086013650521636, val_acc: 1.0\n",
      "epoch: 75, train_loss: 0.0002869369634499326, train_acc: 1.0, val_loss: 0.0005062387208454311, val_acc: 0.9997260273972602\n",
      "epoch: 76, train_loss: 0.0002772462755182664, train_acc: 1.0, val_loss: 0.00035120511893182993, val_acc: 1.0\n",
      "epoch: 77, train_loss: 0.0002674927729472532, train_acc: 1.0, val_loss: 0.0005731359706260264, val_acc: 0.9997260273972602\n",
      "epoch: 78, train_loss: 0.0002727462811298062, train_acc: 1.0, val_loss: 0.0004814080020878464, val_acc: 0.9997260273972602\n",
      "epoch: 79, train_loss: 0.000301431035717711, train_acc: 1.0, val_loss: 0.00043705705320462584, val_acc: 1.0\n",
      "epoch: 80, train_loss: 0.0002840855694691788, train_acc: 1.0, val_loss: 0.00035171231138519943, val_acc: 1.0\n",
      "epoch: 81, train_loss: 0.0002838455064108509, train_acc: 1.0, val_loss: 0.00047580519458279014, val_acc: 1.0\n",
      "epoch: 82, train_loss: 0.00031735585014743716, train_acc: 1.0, val_loss: 0.0005369714926928282, val_acc: 1.0\n",
      "epoch: 83, train_loss: 0.0003617550069312818, train_acc: 1.0, val_loss: 0.00041617199894972146, val_acc: 1.0\n",
      "epoch: 84, train_loss: 0.013104116603688792, train_acc: 0.997869101978691, val_loss: 0.006683217827230692, val_acc: 0.9991780821917808\n",
      "epoch: 85, train_loss: 0.0036518101146365265, train_acc: 0.9996347031963471, val_loss: 0.0012598393950611353, val_acc: 0.9997260273972602\n",
      "epoch: 86, train_loss: 0.0007083134739452164, train_acc: 1.0, val_loss: 0.0007727922056801617, val_acc: 1.0\n",
      "epoch: 87, train_loss: 0.0005574797804904163, train_acc: 1.0, val_loss: 0.0006557600572705269, val_acc: 0.9997260273972602\n",
      "epoch: 88, train_loss: 0.0005276544218568868, train_acc: 1.0, val_loss: 0.0005438106600195169, val_acc: 1.0\n",
      "epoch: 89, train_loss: 0.0003532601320527696, train_acc: 1.0, val_loss: 0.0005181662854738533, val_acc: 1.0\n",
      "epoch: 90, train_loss: 0.00031979259689222993, train_acc: 1.0, val_loss: 0.0004903982044197619, val_acc: 0.9997260273972602\n",
      "epoch: 91, train_loss: 0.0002875303220799799, train_acc: 1.0, val_loss: 0.00039609710802324116, val_acc: 1.0\n",
      "epoch: 92, train_loss: 0.00028993604741286657, train_acc: 1.0, val_loss: 0.00034102951758541167, val_acc: 1.0\n",
      "epoch: 93, train_loss: 0.0006811823024705067, train_acc: 0.9998782343987823, val_loss: 0.0022114317398518324, val_acc: 0.9997260273972602\n",
      "epoch: 94, train_loss: 0.0007624524761020214, train_acc: 0.9999695585996956, val_loss: 0.0005039694951847196, val_acc: 1.0\n",
      "epoch: 95, train_loss: 0.0003263074286505869, train_acc: 1.0, val_loss: 0.0004543290415313095, val_acc: 0.9997260273972602\n",
      "epoch: 96, train_loss: 0.0029228071638386583, train_acc: 0.9995433789954338, val_loss: 0.004700274206697941, val_acc: 0.9997260273972602\n",
      "epoch: 97, train_loss: 0.001330752087244403, train_acc: 1.0, val_loss: 0.000500025344081223, val_acc: 1.0\n",
      "epoch: 98, train_loss: 0.002647196415693557, train_acc: 0.9993911719939117, val_loss: 0.002679189434275031, val_acc: 1.0\n",
      "epoch: 99, train_loss: 0.0008811764429231326, train_acc: 1.0, val_loss: 0.0006716118659824133, val_acc: 1.0\n",
      "epoch: 100, train_loss: 0.00038359470743516304, train_acc: 1.0, val_loss: 0.00043898180592805147, val_acc: 1.0\n",
      "epoch: 101, train_loss: 0.000273775412202921, train_acc: 1.0, val_loss: 0.000393958151107654, val_acc: 1.0\n",
      "epoch: 102, train_loss: 0.00026737607527314866, train_acc: 1.0, val_loss: 0.00039295005262829363, val_acc: 1.0\n",
      "epoch: 103, train_loss: 0.00024698389726357045, train_acc: 1.0, val_loss: 0.00046231155283749104, val_acc: 1.0\n",
      "epoch: 104, train_loss: 0.00022436022202803775, train_acc: 1.0, val_loss: 0.00042927375761792064, val_acc: 1.0\n",
      "epoch: 105, train_loss: 0.0002183849587039336, train_acc: 1.0, val_loss: 0.0003617009788285941, val_acc: 1.0\n",
      "epoch: 106, train_loss: 0.00023836854397673015, train_acc: 1.0, val_loss: 0.0004135245399083942, val_acc: 1.0\n",
      "epoch: 107, train_loss: 0.0002176831453829253, train_acc: 1.0, val_loss: 0.0004649392794817686, val_acc: 0.9997260273972602\n",
      "epoch: 108, train_loss: 0.00022467571448772972, train_acc: 1.0, val_loss: 0.0003812782815657556, val_acc: 1.0\n",
      "epoch: 109, train_loss: 0.00024150485159782332, train_acc: 1.0, val_loss: 0.0004210231709294021, val_acc: 1.0\n",
      "epoch: 110, train_loss: 0.00035685001497514284, train_acc: 1.0, val_loss: 0.0003721429966390133, val_acc: 1.0\n",
      "epoch: 111, train_loss: 0.0002513730373051263, train_acc: 1.0, val_loss: 0.0003419123822823167, val_acc: 1.0\n",
      "epoch: 112, train_loss: 0.0002768982769000929, train_acc: 1.0, val_loss: 0.0003431834338698536, val_acc: 1.0\n",
      "epoch: 113, train_loss: 0.00025318504607597323, train_acc: 1.0, val_loss: 0.0003939181624446064, val_acc: 1.0\n",
      "epoch: 114, train_loss: 0.0002791923435825436, train_acc: 1.0, val_loss: 0.0003934068663511425, val_acc: 1.0\n",
      "epoch: 115, train_loss: 0.0003964176229805895, train_acc: 1.0, val_loss: 0.0004958143690600991, val_acc: 0.9997260273972602\n",
      "epoch: 116, train_loss: 0.0005208794816077665, train_acc: 0.9999695585996956, val_loss: 0.02302064746618271, val_acc: 0.9942465753424657\n",
      "epoch: 117, train_loss: 0.011176277101834038, train_acc: 0.9982648401826484, val_loss: 0.0025032975245267153, val_acc: 0.9997260273972602\n",
      "epoch: 118, train_loss: 0.0017585208393135216, train_acc: 0.9999086757990867, val_loss: 0.0009004063904285431, val_acc: 1.0\n",
      "epoch: 119, train_loss: 0.0006432337679651154, train_acc: 1.0, val_loss: 0.00047660438576713204, val_acc: 1.0\n",
      "epoch: 120, train_loss: 0.00042241311414607677, train_acc: 1.0, val_loss: 0.0004318913270253688, val_acc: 1.0\n",
      "epoch: 121, train_loss: 0.000327862933631842, train_acc: 1.0, val_loss: 0.0004238043329678476, val_acc: 1.0\n",
      "epoch: 122, train_loss: 0.00030631340796248845, train_acc: 1.0, val_loss: 0.0003209836722817272, val_acc: 1.0\n",
      "epoch: 123, train_loss: 0.0002401893930924811, train_acc: 1.0, val_loss: 0.0003865104808937758, val_acc: 1.0\n",
      "epoch: 124, train_loss: 0.00024251358415714876, train_acc: 1.0, val_loss: 0.00035516059142537415, val_acc: 1.0\n",
      "epoch: 125, train_loss: 0.00022964849682297384, train_acc: 1.0, val_loss: 0.00037182527012191713, val_acc: 1.0\n",
      "epoch: 126, train_loss: 0.00023394220072303198, train_acc: 1.0, val_loss: 0.0005129684577696025, val_acc: 1.0\n",
      "epoch: 127, train_loss: 0.00025098869759118355, train_acc: 1.0, val_loss: 0.00036103633465245366, val_acc: 1.0\n",
      "epoch: 128, train_loss: 0.0002407810767858051, train_acc: 1.0, val_loss: 0.00037074528518132865, val_acc: 1.0\n",
      "epoch: 129, train_loss: 0.00027598627520790105, train_acc: 1.0, val_loss: 0.00038375818985514343, val_acc: 0.9997260273972602\n",
      "epoch: 130, train_loss: 0.0002566376441995823, train_acc: 1.0, val_loss: 0.00034254093770869076, val_acc: 1.0\n",
      "epoch: 131, train_loss: 0.0002464845980961569, train_acc: 1.0, val_loss: 0.0003809437621384859, val_acc: 1.0\n",
      "epoch: 132, train_loss: 0.0002451527809255843, train_acc: 1.0, val_loss: 0.00038136797957122326, val_acc: 1.0\n",
      "epoch: 133, train_loss: 0.0002542570734878482, train_acc: 1.0, val_loss: 0.0003886698978021741, val_acc: 1.0\n",
      "epoch: 134, train_loss: 0.0002508606339688413, train_acc: 1.0, val_loss: 0.0006559056928381324, val_acc: 1.0\n",
      "epoch: 135, train_loss: 0.0002952511411055547, train_acc: 1.0, val_loss: 0.00035629310877993703, val_acc: 1.0\n",
      "epoch: 136, train_loss: 0.0002708137964887559, train_acc: 1.0, val_loss: 0.0004118034557905048, val_acc: 1.0\n",
      "epoch: 137, train_loss: 0.00028650484161989317, train_acc: 1.0, val_loss: 0.0003675572806969285, val_acc: 1.0\n",
      "epoch: 138, train_loss: 0.00029815820620175047, train_acc: 1.0, val_loss: 0.00045904499711468816, val_acc: 1.0\n",
      "epoch: 139, train_loss: 0.0002905929487292445, train_acc: 1.0, val_loss: 0.0003461583110038191, val_acc: 1.0\n",
      "epoch: 140, train_loss: 0.00042701833608448363, train_acc: 1.0, val_loss: 0.00046992514398880303, val_acc: 1.0\n",
      "epoch: 141, train_loss: 0.008077630383669139, train_acc: 0.9990258751902588, val_loss: 0.005659017711877823, val_acc: 1.0\n",
      "epoch: 142, train_loss: 0.0015388936796604635, train_acc: 1.0, val_loss: 0.0006557480082847178, val_acc: 0.9997260273972602\n",
      "epoch: 143, train_loss: 0.0004660208168522592, train_acc: 1.0, val_loss: 0.0004947270499542356, val_acc: 1.0\n",
      "epoch: 144, train_loss: 0.00034140182077855793, train_acc: 1.0, val_loss: 0.00044595682993531227, val_acc: 0.9997260273972602\n",
      "epoch: 145, train_loss: 0.0002944160732458699, train_acc: 1.0, val_loss: 0.000381166348233819, val_acc: 1.0\n",
      "epoch: 146, train_loss: 0.0002707002848944475, train_acc: 1.0, val_loss: 0.0003580983029678464, val_acc: 1.0\n",
      "epoch: 147, train_loss: 0.0002388842905873375, train_acc: 1.0, val_loss: 0.0003262086247559637, val_acc: 1.0\n",
      "epoch: 148, train_loss: 0.00023957721133447475, train_acc: 1.0, val_loss: 0.00031965094967745245, val_acc: 1.0\n",
      "epoch: 149, train_loss: 0.00022987655613051597, train_acc: 1.0, val_loss: 0.0004665782325901091, val_acc: 1.0\n",
      "epoch: 150, train_loss: 0.00026671529423568193, train_acc: 1.0, val_loss: 0.0003615943423938006, val_acc: 1.0\n",
      "epoch: 151, train_loss: 0.00022903941033630047, train_acc: 1.0, val_loss: 0.00034357295953668654, val_acc: 1.0\n",
      "epoch: 152, train_loss: 0.0002408774709283887, train_acc: 1.0, val_loss: 0.0003316699876450002, val_acc: 1.0\n",
      "epoch: 153, train_loss: 0.00023782790400261102, train_acc: 1.0, val_loss: 0.0008383545209653676, val_acc: 0.9997260273972602\n",
      "epoch: 154, train_loss: 0.000252862336599564, train_acc: 1.0, val_loss: 0.00034682804835028946, val_acc: 1.0\n",
      "epoch: 155, train_loss: 0.0002558129035746823, train_acc: 1.0, val_loss: 0.00034256099024787545, val_acc: 1.0\n",
      "epoch: 156, train_loss: 0.0002453349566343421, train_acc: 1.0, val_loss: 0.00035058066714555025, val_acc: 1.0\n",
      "epoch: 157, train_loss: 0.0002595030664522273, train_acc: 1.0, val_loss: 0.00027824094286188483, val_acc: 1.0\n",
      "epoch: 158, train_loss: 0.0002710811309642772, train_acc: 1.0, val_loss: 0.00035676907282322645, val_acc: 1.0\n",
      "epoch: 159, train_loss: 0.00029120109221796947, train_acc: 1.0, val_loss: 0.0004416715237312019, val_acc: 1.0\n",
      "epoch: 160, train_loss: 0.00029036464474647657, train_acc: 1.0, val_loss: 0.00031435859273187816, val_acc: 1.0\n",
      "epoch: 161, train_loss: 0.0003037668098812894, train_acc: 1.0, val_loss: 0.0003947493387386203, val_acc: 1.0\n",
      "epoch: 162, train_loss: 0.00028874574395170967, train_acc: 1.0, val_loss: 0.00035725609632208943, val_acc: 1.0\n",
      "epoch: 163, train_loss: 0.0003154750173602816, train_acc: 1.0, val_loss: 0.00038330379175022244, val_acc: 1.0\n",
      "epoch: 164, train_loss: 0.010084463042918188, train_acc: 0.9982039573820396, val_loss: 0.002838411834090948, val_acc: 0.9997260273972602\n",
      "epoch: 165, train_loss: 0.001729108443277953, train_acc: 0.9998782343987823, val_loss: 0.0007035809103399515, val_acc: 1.0\n",
      "epoch: 166, train_loss: 0.00044204806141887406, train_acc: 1.0, val_loss: 0.00047538927174173295, val_acc: 1.0\n",
      "epoch: 167, train_loss: 0.0003096896628470182, train_acc: 1.0, val_loss: 0.00034417147981002927, val_acc: 1.0\n",
      "epoch: 168, train_loss: 0.0002879997291881952, train_acc: 1.0, val_loss: 0.00033904187148436904, val_acc: 1.0\n",
      "epoch: 169, train_loss: 0.00025666837948022496, train_acc: 1.0, val_loss: 0.0003769333416130394, val_acc: 1.0\n",
      "epoch: 170, train_loss: 0.0002629963122488743, train_acc: 1.0, val_loss: 0.00035529874730855227, val_acc: 1.0\n",
      "epoch: 171, train_loss: 0.00023746215255548384, train_acc: 1.0, val_loss: 0.00034959460026584566, val_acc: 1.0\n",
      "epoch: 172, train_loss: 0.00022481290892925287, train_acc: 1.0, val_loss: 0.0003009790671057999, val_acc: 1.0\n",
      "epoch: 173, train_loss: 0.00022031595639870031, train_acc: 1.0, val_loss: 0.0002975550014525652, val_acc: 1.0\n",
      "epoch: 174, train_loss: 0.0012424349457263774, train_acc: 0.9998782343987823, val_loss: 0.0010689907940104604, val_acc: 1.0\n",
      "epoch: 175, train_loss: 0.0004710151812570646, train_acc: 1.0, val_loss: 0.0004407137166708708, val_acc: 1.0\n",
      "epoch: 176, train_loss: 0.00025174934633595997, train_acc: 1.0, val_loss: 0.00034803865128196776, val_acc: 1.0\n",
      "epoch: 177, train_loss: 0.00022578744730793025, train_acc: 1.0, val_loss: 0.00029436187469400465, val_acc: 1.0\n",
      "epoch: 178, train_loss: 0.00022484882288682816, train_acc: 1.0, val_loss: 0.00032654142705723643, val_acc: 1.0\n",
      "epoch: 179, train_loss: 0.00022450212683427587, train_acc: 1.0, val_loss: 0.00032170486520044506, val_acc: 1.0\n",
      "epoch: 180, train_loss: 0.0002247499878578747, train_acc: 1.0, val_loss: 0.00038893544115126133, val_acc: 1.0\n",
      "epoch: 181, train_loss: 0.00022794712609439986, train_acc: 1.0, val_loss: 0.00036221800837665796, val_acc: 1.0\n",
      "epoch: 182, train_loss: 0.00023965062805931554, train_acc: 1.0, val_loss: 0.0003972597769461572, val_acc: 1.0\n",
      "epoch: 183, train_loss: 0.00024463058082258704, train_acc: 1.0, val_loss: 0.000365531537681818, val_acc: 1.0\n",
      "epoch: 184, train_loss: 0.0002720329680167817, train_acc: 1.0, val_loss: 0.00036542295129038393, val_acc: 1.0\n",
      "epoch: 185, train_loss: 0.00027327909668575426, train_acc: 1.0, val_loss: 0.0003904623445123434, val_acc: 1.0\n",
      "epoch: 186, train_loss: 0.0002867271281037003, train_acc: 1.0, val_loss: 0.00042901630513370037, val_acc: 1.0\n",
      "epoch: 187, train_loss: 0.00028921925131585307, train_acc: 1.0, val_loss: 0.000361602840712294, val_acc: 1.0\n",
      "epoch: 188, train_loss: 0.0002887634247543232, train_acc: 1.0, val_loss: 0.0004417242016643286, val_acc: 1.0\n",
      "epoch: 189, train_loss: 0.00030540547307938076, train_acc: 1.0, val_loss: 0.0003841443976853043, val_acc: 1.0\n",
      "epoch: 190, train_loss: 0.008028316082994187, train_acc: 0.9986605783866058, val_loss: 0.011492813006043434, val_acc: 0.9983561643835617\n",
      "epoch: 191, train_loss: 0.0053069338364014045, train_acc: 0.9995129375951294, val_loss: 0.0015081694582477212, val_acc: 1.0\n",
      "epoch: 192, train_loss: 0.0007644217053442732, train_acc: 1.0, val_loss: 0.0005711776902899146, val_acc: 1.0\n",
      "epoch: 193, train_loss: 0.0004301937550799055, train_acc: 1.0, val_loss: 0.00042959622805938125, val_acc: 1.0\n",
      "epoch: 194, train_loss: 0.00032643362015211423, train_acc: 1.0, val_loss: 0.0003661797381937504, val_acc: 1.0\n",
      "epoch: 195, train_loss: 0.00029285438956653105, train_acc: 1.0, val_loss: 0.0003513314004521817, val_acc: 1.0\n",
      "epoch: 196, train_loss: 0.00028160437632464015, train_acc: 1.0, val_loss: 0.0003115357249043882, val_acc: 1.0\n",
      "epoch: 197, train_loss: 0.0002475528349033232, train_acc: 1.0, val_loss: 0.00033136497950181365, val_acc: 1.0\n",
      "epoch: 198, train_loss: 0.00024327926239352528, train_acc: 1.0, val_loss: 0.00035391777055338025, val_acc: 1.0\n",
      "epoch: 199, train_loss: 0.00023787193838010592, train_acc: 1.0, val_loss: 0.000310113508021459, val_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_set, val_set = loadData('data')\n",
    "    train_dl = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_dl = DataLoader(dataset=val_set, batch_size=len(val_set), num_workers=2)\n",
    "\n",
    "    device = get_device()\n",
    "    model = mymodel().to(device)\n",
    "    train_model(model, train_dl, val_dl, device, 'savedModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_file = open(\"targets.pkl\", \"rb\")\n",
    "# tmp = pickle.load(target_file)\n",
    "# target_file.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
